{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using the Google News Model\n",
    "#not pushed because it is too big but can be found here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "model = Word2Vec.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I will be using the first folder as my data\n",
    "queries_file = open(\"../data/user_studies/all.txt\", \"r\")\n",
    "lines = queries_file.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "# remove all strings with length < 3\n",
    "queries = []\n",
    "for i in lines:\n",
    "    if not len(i)<3:\n",
    "        queries.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def get_sentence_vector(q):\n",
    "    \"\"\"returns the vector for a sentence\"\"\"\n",
    "    #remove all punctuation from query\n",
    "    q = re.sub(r'[^\\w\\s]','',q)\n",
    "    #split by word\n",
    "    q = q.split(\" \")\n",
    "    # remove extra spaces\n",
    "    q = [i.strip() for i in q]\n",
    "    query_vec = np.zeros(300)\n",
    "    #adds the vectors of all individual words to get\n",
    "    for w in q:\n",
    "        try:\n",
    "            query_vec+=model[w]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return query_vec\n",
    "\n",
    "#Get results based on query number\n",
    "def get_results(q_num):\n",
    "    filename = \"../data/user_studies/results_\"+str(q_num)+\".txt\"\n",
    "    f = open(filename, \"r\")\n",
    "    lst = f.readlines()\n",
    "    lst = [i.split(\"--\") for i in lst]\n",
    "    lst = list(zip(*lst))\n",
    "    scores = [float(i.strip()) for i in lst[0]]\n",
    "    results = [i.strip() for i in lst[1]]\n",
    "    return scores, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_number=3\n",
    "query = queries[query_number]\n",
    "scores, results = get_results(query_number)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distribution of the length of the sentences in the query results\n",
    "sentence_lengths = np.array([len(x.split(\" \")) for x in results])\n",
    "plt.title(\"Sentence Length (Words)\")\n",
    "plt.hist(sentence_lengths, bins=50)\n",
    "plt.show()\n",
    "print(\"Percent of sentences with word length greater than 50 is\", \n",
    "      np.count_nonzero(sentence_lengths>50)/len(sentence_lengths)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a distance matrix in which you take pairwise similairties between\n",
    "dist_matrix = np.zeros([1000, 1000])\n",
    "vectors  = [get_sentence_vector(x) for x in results]\n",
    "dist_matrix = 1-cosine_similarity(vectors, vectors) # 1-cosine similarity to be more intuitive when using as a distance\n",
    "dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "il1 = np.tril(dist_matrix, -1)\n",
    "lower_triangle = dist_matrix[il1>0]\n",
    "lower_triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distribution of distances for the first 1000 query results\n",
    "plt.hist(lower_triangle, bins=50, normed=True)\n",
    "plt.title(\"Distances Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pick a small epsilon to draw an edge between results\n",
    "epsilon = np.percentile(lower_triangle, 3)\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a list of edges from the adjacency matrix\n",
    "import networkx as nx\n",
    "edges = []\n",
    "adj_matrix = dist_matrix<epsilon #make adjacency matrix\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        if adj_matrix[i, j] and i!=j:\n",
    "            edges.append((i,j))\n",
    "G = nx.Graph(edges)\n",
    "#number of connected components in Graph\n",
    "len(list(nx.connected_components(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#investigate the degrees of each node\n",
    "G.degree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove all the vertices with a degree of 80 or more \n",
    "low_degree = [x  for x, v in G.degree().items() if v<80]\n",
    "H = G.subgraph(low_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes json which is important for graph visualization\n",
    "# Now you can go and fire up index.html in the browser\n",
    "import json\n",
    "\n",
    "graph = {}\n",
    "graph['nodes'] = [{'id': node, 'group': 1, 'text':results[node]} for node in H.nodes()]\n",
    "graph['links'] = [{'source': i, 'target': j, 'value': 1} for (i,j) in H.edges()]\n",
    "\n",
    "   \n",
    "f = open('graph.json', 'w')\n",
    "f.write(json.dumps(graph))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#number of edges and vertices in subgraph\n",
    "len(H.edges()), len(H.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding meaning within clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "#this looks more like the partition the the code in js gives as opposed to best_partition\n",
    "d = community_louvain.generate_dendrogram(H)\n",
    "partition = community_louvain.partition_at_level(d, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make an array where in the ith position, we have a list of all the nodes belonging to that community\n",
    "communities = [[]]*(max(partition.values())+1)\n",
    "for node, com in partition.items():\n",
    "    communities[com] = communities[com]+[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#community sizes\n",
    "import pandas as pd\n",
    "import nltk\n",
    "pd.Series([len(x) for x in communities]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#try to find some common keywords within communities\n",
    "com5 = communities[4] #community of size 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_in_community = np.array(results)[com5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def clean_sentence(s):\n",
    "    \"\"\"Returns a list of all the words in a sentence with no stop words and no punctuation\"\"\"\n",
    "    tokenized_sentence = [i.strip() for i in nltk.word_tokenize(s.lower())]\n",
    "    words = [w for w in tokenized_sentence if w not in stopwords.words('english') and w not in punctuation]\n",
    "    return words\n",
    "\n",
    "s1 = clean_sentence(sentences_in_community[0])\n",
    "s2 = clean_sentence(sentences_in_community[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
