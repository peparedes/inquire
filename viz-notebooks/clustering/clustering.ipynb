{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using the Google News Model\n",
    "#not pushed because it is too big but can be found here: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "model = Word2Vec.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#I will be using the first folder as my data\n",
    "queries_file = open(\"../data/user_studies/all.txt\", \"r\")\n",
    "lines = queries_file.readlines()\n",
    "lines = [l.strip() for l in lines]\n",
    "# remove all strings with length < 3\n",
    "queries = []\n",
    "for i in lines:\n",
    "    if not len(i)<3:\n",
    "        queries.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def get_sentence_vector(q):\n",
    "    \"\"\"returns the vector for a sentence\"\"\"\n",
    "    #remove all punctuation from query\n",
    "    q = re.sub(r'[^\\w\\s]','',q)\n",
    "    #split by word\n",
    "    q = q.split(\" \")\n",
    "    # remove extra spaces\n",
    "    q = [i.strip() for i in q]\n",
    "    query_vec = np.zeros(300)\n",
    "    #adds the vectors of all individual words to get\n",
    "    for w in q:\n",
    "        try:\n",
    "            query_vec+=model[w]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return query_vec\n",
    "\n",
    "#Get results based on query number\n",
    "def get_results(q_num):\n",
    "    filename = \"../data/user_studies/results_\"+str(q_num)+\".txt\"\n",
    "    f = open(filename, \"r\")\n",
    "    lst = f.readlines()\n",
    "    lst = [i.split(\"--\") for i in lst]\n",
    "    lst = list(zip(*lst))\n",
    "    scores = [float(i.strip()) for i in lst[0]]\n",
    "    results = [i.strip() for i in lst[1]]\n",
    "    return scores, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query_number=3\n",
    "query = queries[query_number]\n",
    "scores, results = get_results(query_number)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distribution of the length of the sentences in the query results\n",
    "sentence_lengths = np.array([len(x.split(\" \")) for x in results])\n",
    "plt.title(\"Sentence Length (Words)\")\n",
    "plt.hist(sentence_lengths, bins=50)\n",
    "plt.show()\n",
    "print(\"Percent of sentences with word length greater than 50 is\", \n",
    "      np.count_nonzero(sentence_lengths>50)/len(sentence_lengths)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a distance matrix in which you take pairwise similairties between\n",
    "dist_matrix = np.zeros([1000, 1000])\n",
    "vectors  = [get_sentence_vector(x) for x in results]\n",
    "dist_matrix = 1-cosine_similarity(vectors, vectors) # 1-cosine similarity to be more intuitive when using as a distance\n",
    "dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "il1 = np.tril(dist_matrix, -1)\n",
    "lower_triangle = dist_matrix[il1>0]\n",
    "lower_triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#distribution of distances for the first 1000 query results\n",
    "plt.hist(lower_triangle, bins=50, normed=True)\n",
    "plt.title(\"Distances Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pick a small epsilon to draw an edge between results\n",
    "epsilon = np.percentile(lower_triangle, 3)\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a list of edges from the adjacency matrix\n",
    "import networkx as nx\n",
    "edges = []\n",
    "adj_matrix = dist_matrix<epsilon #make adjacency matrix\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        if adj_matrix[i, j] and i!=j:\n",
    "            edges.append((i,j))\n",
    "G = nx.Graph(edges)\n",
    "#number of connected components in Graph\n",
    "len(list(nx.connected_components(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove all the vertices with a degree of 80 or more \n",
    "low_degree = [x  for x, v in G.degree().items() if v<80]\n",
    "high_degree = [x  for x, v in G.degree().items() if v>=80]\n",
    "most_connected_sentences = np.array(results)[high_degree]\n",
    "H = G.subgraph(low_degree)\n",
    "len(high_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes json which is important for graph visualization\n",
    "# Now you can go and fire up index.html in the browser\n",
    "import json\n",
    "\n",
    "graph = {}\n",
    "graph['nodes'] = [{'id': node, 'group': 1, 'text':results[node]} for node in H.nodes()]\n",
    "graph['links'] = [{'source': i, 'target': j, 'value': 1} for (i,j) in H.edges()]\n",
    "\n",
    "   \n",
    "f = open('graph.json', 'w')\n",
    "f.write(json.dumps(graph))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#number of edges and vertices in subgraph\n",
    "len(H.edges()), len(H.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding meaning within clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from community import community_louvain\n",
    "#this looks more like the partition the the code in js gives as opposed to best_partition\n",
    "d = community_louvain.generate_dendrogram(H)\n",
    "partition = community_louvain.partition_at_level(d, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make an array where in the ith position, we have a list of all the nodes belonging to that community\n",
    "communities = [[]]*(max(partition.values())+1)\n",
    "for node, com in partition.items():\n",
    "    communities[com] = communities[com]+[node]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#community sizes\n",
    "import pandas as pd\n",
    "import nltk\n",
    "pd.Series([len(x) for x in communities]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#try to find some common keywords within communities\n",
    "com5 = communities[4] #community of size 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_in_community = np.array(results)[com5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import itertools\n",
    "\n",
    "def clean_sentence(s):\n",
    "    \"\"\"Returns a list of all the words in a sentence with no stop words and no punctuation\"\"\"\n",
    "    tokenized_sentence = [i.strip() for i in nltk.word_tokenize(s.lower())]\n",
    "    words = [w for w in tokenized_sentence if w not in stopwords.words('english') and w not in punctuation]\n",
    "    return words\n",
    "\n",
    "#Find most relevant words\n",
    "\n",
    "def make_dictionary(docs):\n",
    "    \"\"\"Takes in a list of documents and returns an alphabetically sorted list of all distinct words\"\"\"\n",
    "    s = list(set(list(itertools.chain.from_iterable(docs))))\n",
    "    s = list(sorted(s))\n",
    "    return dict([(s[i], i) for i in range(len(s))])\n",
    "\n",
    "def count_instances(map_, doc):\n",
    "    \"\"\"Counts instances words in docs given a dictionary\"\"\"\n",
    "    count_array = np.zeros(len(map_.keys()))\n",
    "    for word, index in map_.items():\n",
    "        count_array[index] = doc.count(word)+1\n",
    "    return count_array\n",
    "\n",
    "def find_ngrams(input_list, n):\n",
    "      if type(n)==list:\n",
    "            lst = []\n",
    "            for i in n:\n",
    "                lst.extend(list(zip(*[input_list[j:] for j in range(i)])))\n",
    "            return lst\n",
    "      return list(zip(*[input_list[i:] for i in range(n)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find most common words for cluster #4\n",
    "sentences_in_cluster = np.array(results)[communities[4]]\n",
    "ngrams = [find_ngrams(clean_sentence(i), [4,5]) for i in sentences_in_cluster]\n",
    "dictionary = make_dictionary(ngrams)\n",
    "a = np.ones(len(dictionary))\n",
    "for s in ngrams:\n",
    "    a+=count_instances(dictionary, s)\n",
    "best_scored = np.argsort(a)[::-1]\n",
    "rev_dict = dict(map(reversed, dictionary.items()))\n",
    "[rev_dict[i] for i in best_scored[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find most common words for cluster #5\n",
    "sentences_in_cluster = np.array(results)[communities[5]]\n",
    "ngrams = [find_ngrams(clean_sentence(i), [4,5]) for i in sentences_in_cluster]\n",
    "dictionary = make_dictionary(ngrams)\n",
    "a = np.ones(len(dictionary))\n",
    "for s in ngrams:\n",
    "    a+=count_instances(dictionary, s)\n",
    "best_scored = np.argsort(a)[::-1]\n",
    "rev_dict = dict(map(reversed, dictionary.items()))\n",
    "[rev_dict[i] for i in best_scored[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn to the rescue + wordcloud implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "analyze = vectorizer.build_analyzer()\n",
    "\n",
    "def top_ngrams(communities,cluster, n=20):\n",
    "    \"\"\"Input:\n",
    "        communities: List of lists that contain indices corresponding to sentences in the original array\n",
    "        cluster: integer, the cluster to inspect\n",
    "        n: integer, number of most common \"\"\"\n",
    "    sentences = np.array(results)[communities[cluster]]\n",
    "    ngrams = []\n",
    "    for s in sentences:\n",
    "        ngrams.extend(analyze(s))\n",
    "\n",
    "    counts = Counter(ngrams)\n",
    "\n",
    "    return sorted(counts.items(), key=operator.itemgetter(1))[::-1][:n]\n",
    "\n",
    "def get_wordcloud_json(ngram):\n",
    "    wordcloud = []\n",
    "    for word, count in ngram:\n",
    "        wordcloud.append({\"text\":word, \"size\":count})\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordcloud = get_wordcloud_json(top_ngrams(communities,8, n=50))\n",
    "f = open('wordcloud.json', 'w')\n",
    "f.write(json.dumps(wordcloud))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## Top verbs with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clustering using verbs\n",
    "def get_verb_counts(cluster):\n",
    "    \"\"\"Returns a dictionary that maps words to their count\"\"\"\n",
    "    com = np.array(results)[communities[cluster]]\n",
    "    verbs = {}\n",
    "    for post in com:\n",
    "        doc = nlp(str(post))\n",
    "        for sentence in doc.sents:\n",
    "            if sentence.root.lemma_ not in verbs.keys():\n",
    "                verbs[sentence.root.lemma_] = 1\n",
    "            else:\n",
    "                verbs[sentence.root.lemma_] += 1\n",
    "    return verbs\n",
    "\n",
    "# def find_different_elements(s1, s2):\n",
    "#     \"\"\"Given two lists, returns two lists with the sets minus their intersection\"\"\"\n",
    "#     s1 = set(s1)\n",
    "#     s2 = set(s2)\n",
    "#     return list(s1.difference(s1.intersection(s2))), list(s2.difference(s1.intersection(s2))) \n",
    "\n",
    "def find_top_counts(dict_, n=10):\n",
    "    \"\"\"Given a dictionary of verb frequencies, returns the n most common verbs\"\"\"\n",
    "    return [u for u,v in sorted(dict_.items(), key=operator.itemgetter(1))][::-1][:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top adjectives with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_adj_counts(cluster):\n",
    "    \"\"\"Returns a dictionary that maps words to their count\"\"\"\n",
    "    com = np.array(results)[communities[cluster]]\n",
    "    adjs = {}\n",
    "    for post in com:\n",
    "        doc = nlp(str(post))\n",
    "        for sentence in doc.sents:\n",
    "            for w in sentence:\n",
    "                if w.pos_ == 'ADJ' and w.dep_ == 'amod':\n",
    "                    if w.lemma_ not in adjs.keys():\n",
    "                        adjs[w.lemma_] = 1\n",
    "                    else:\n",
    "                        adjs[w.lemma_]+=1\n",
    "    return adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get tags for all clusters that have a size bigger than 20\n",
    "tags = []\n",
    "for c in range(len(communities)):\n",
    "    d = {'cluster':c, 'top_verbs':[], 'verb_counts':[], 'adjs_counts':[], 'top_adjs':[]}\n",
    "    if len(communities[c])>20:\n",
    "        verb_counts= get_verb_counts(c)\n",
    "        top_verbs = find_top_counts(verb_counts, 20)\n",
    "        top_verb_counts = [verb_counts[x] for x in top_verbs]\n",
    "        d['top_verbs'] = top_verbs\n",
    "        d['verb_counts'] = top_verb_counts\n",
    "        adj_counts = get_adj_counts(c)\n",
    "        top_adj = find_top_counts(adj_counts, 20)\n",
    "        top_adj_counts = [adj_counts[x] for x in top_adj]\n",
    "        d['top_adjs'] = top_adj\n",
    "        d['top_adjs_counts'] = top_adj_counts\n",
    "    tags.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using clustering from python and using verb as text that appears\n",
    "import json\n",
    "\n",
    "graph = {}\n",
    "graph['nodes'] = [{'id': node, 'group': partition[node], 'text':\"Top verbs: \"+\n",
    "                   \", \".join(tags[partition[node]]['top_verbs'][-5:])\n",
    "                            +\"\\n\" + \"Top adjectives: \"+\n",
    "                             \", \".join(tags[partition[node]]['top_adjs'])} for node in H.nodes()]\n",
    "graph['links'] = [{'source': i, 'target': j, 'value': 1} for (i,j) in H.edges()]\n",
    "    \n",
    "\n",
    "f = open('graph.json', 'w')\n",
    "f.write(json.dumps(graph))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1. look at adjectives\n",
    "2. modifiers of adjectives, tree dependencies on spacy\n",
    "unicode\n",
    "3. lemmatize verbs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
